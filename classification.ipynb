{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSIFICATION**\n",
    "\n",
    "Welcome dear reader. In this jupyter-notebook, you will meet the classification of nothing less but glass. Have you ever wondered about how a computer would classify your bedroom window? Wonder no more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from toolbox_02450 import rocplot, confmatplot\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import time\n",
    "\n",
    "font_size = 15\n",
    "plt.rcParams.update({'font.size': font_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "filename = 'data/glass.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# defining x and y for the linear regression\n",
    "#       all attributes except RI\n",
    "X = np.array(df.iloc[:,1:10])\n",
    "#       number of attributes\n",
    "M = 9\n",
    "#       type\n",
    "y = np.array(df.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# the format of C needs to be this, so it is usable with torch, see before and after:\n",
    "print(y)\n",
    "y[y>3] = y[y>3] - 1\n",
    "y = y - 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 1/10. inner_loop: 2/10.  Time pr. iteration: 28 sec. Estimated time left: 47 min.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 1/10. inner_loop: 3/10.  Time pr. iteration: 31 sec. Estimated time left: 51 min.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 1/10. inner_loop: 6/10.  Time pr. iteration: 31 sec. Estimated time left: 49 min.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 1/10. inner_loop: 7/10.  Time pr. iteration: 80 sec. Estimated time left: 129 min.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 1/10. inner_loop: 9/10.  Time pr. iteration: 33 sec. Estimated time left: 49 min.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 6/10. inner_loop: 7/10.  Time pr. iteration: 84 sec. Estimated time left: 63 min...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_loop: 10/10. inner_loop: 10/10.  Time pr. iteration: 89 sec. Estimated time left: 4 min..\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# K = K_outer = K_inner\n",
    "K = 10\n",
    "CV_outer = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "\n",
    "#* INIT FOR LOGISTIC REGRESSION\n",
    "# init for lambda-inner-loop\n",
    "lambda_interval = np.logspace(-8, 2, 10)\n",
    "\n",
    "\n",
    "#* INIT FOR STATISTICAL DATA\n",
    "n_list = np.zeros(10)\n",
    "mc_nemar = []\n",
    "variance = np.zeros(10)\n",
    "\n",
    "\n",
    "# init for error save\n",
    "log_train_error = np.zeros(10)\n",
    "log_test_error = np.zeros(10)\n",
    "\n",
    "opt_lambda = np.zeros(10)\n",
    "#coefficient_norm = np.zeros(len(lambda_interval))\n",
    "\n",
    "\n",
    "#* INIT FOR BASE-LINE\n",
    "base_line_train_err = np.zeros(10)\n",
    "base_line_test_err = np.zeros(10)\n",
    "\n",
    "#* INIT FOR ANN-CLASSIFICATION\n",
    "max_iter = 10000\n",
    "tolerance = 1e-6\n",
    "logging_frequency = 1000\n",
    "best_final_loss = 1e100\n",
    "n_replicates = 1\n",
    "ANN_error_test = np.zeros(K)\n",
    "ANN_error_train = np.zeros(K)\n",
    "\n",
    "h = [i for i in range(1,11)]\n",
    "\n",
    "C = len(np.unique(y))\n",
    "\n",
    "opt_h = np.zeros(10)\n",
    "\n",
    "\n",
    "# init for time taking OPTIONAL\n",
    "\n",
    "time_after_print = 0\n",
    "time_elapsed = 0\n",
    "#* OUTER LOOP\n",
    "for m, (train_index, test_index) in enumerate(CV_outer.split(X,y)):\n",
    "    X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    n_list[m] = len(y_test)\n",
    "\n",
    "    # TODO: CHECK IF THIS STANDARDIZATION IS CORRECT.\n",
    "    mu = np.mean(X_train, axis=0)\n",
    "    sigma = np.std(X_train, axis = 0)\n",
    "\n",
    "    X_train = (X_train - mu)/sigma\n",
    "    X_test = (X_test - mu)/sigma\n",
    "\n",
    "    #init for optimal h\n",
    "    h_error_train = np.zeros((K,len(h)))\n",
    "    h_error_test = np.zeros((K,len(h)))\n",
    "\n",
    "    #init for optimal lambda\n",
    "    lambda_error_train = np.zeros((K,len(lambda_interval)))\n",
    "    lambda_error_test = np.zeros((K,len(lambda_interval)))\n",
    "\n",
    "    #* INNER LOOP\n",
    "    ## ## GOAL: finding best lambda & finding best number of hidden neurons\n",
    "    CV_inner = model_selection.KFold(n_splits=K,shuffle=True)\n",
    "    for j, (train_index_inner, test_index_inner) in enumerate(CV_inner.split(X_train, y_train)):\n",
    "        # printing\n",
    "        \n",
    "        time_before_print = time.time()\n",
    "        time_difference = time_before_print - time_after_print\n",
    "        if (j == 0) & (m == 0):\n",
    "            time_difference = 0\n",
    "        time_elapsed += time_difference\n",
    "        \n",
    "        print(\"outer_loop: \" + str(m+1) + \"/10. inner_loop: \" + str(j+1) + \"/10.  Time pr. iteration: \" + str(round(time_difference)) + \\\n",
    "            \" sec. Estimated time left: \" + str(round((100*time_difference - time_elapsed)/60)) + str(\" min.\"),end='\\r')\n",
    "\n",
    "        time_after_print = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        X_train_inner, X_test_inner = X_train[train_index_inner, :], X_train[test_index_inner, :]\n",
    "        y_train_inner, y_test_inner = y_train[train_index_inner], y_train[test_index_inner]\n",
    "\n",
    "        \n",
    "\n",
    "        ##########################\n",
    "        #* Finding the best lambda:\n",
    "        # iterate over hyperparameter lambda\n",
    "        for k in range(0, len(lambda_interval)):\n",
    "            #'newton-cg' gives better results\n",
    "            mdl = LogisticRegression(penalty='l2', solver='newton-cg', C=1/lambda_interval[k], max_iter=100)\n",
    "            # but the other is faster but outputs hella lot of warnings:\n",
    "            # mdl = LogisticRegression(penalty='l2', C=1/lambda_interval[k], max_iter=100)\n",
    "            \n",
    "            mdl.fit(X_train_inner, y_train_inner)\n",
    "\n",
    "            y_train_est_inner = mdl.predict(X_train_inner).T\n",
    "            y_test_est_inner = mdl.predict(X_test_inner).T\n",
    "            \n",
    "            lambda_error_train[j,k] = np.sum(y_train_est_inner != y_train_inner) / len(y_train_inner)\n",
    "            lambda_error_test[j,k] = np.sum(y_test_est_inner != y_test_inner) / len(y_test_inner)\n",
    "\n",
    "            # føler ikke det her skal bruges\n",
    "            #w_est = mdl.coef_[0] \n",
    "            #coefficient_norm[k] = np.sqrt(np.sum(w_est**2))\n",
    "\n",
    "\n",
    "        #* Finding the optimal amount of hidden layers:\n",
    "        X_train_inner, X_test_inner = torch.tensor(X_train_inner, dtype=torch.float), torch.tensor(X_test_inner, dtype=torch.float)\n",
    "        y_train_inner, y_test_inner = torch.tensor(y_train_inner), torch.tensor(y_test_inner)\n",
    "        # reshaping helps torch\n",
    "        #y_test_inner = torch.reshape(y_test_inner, (y_test_inner.shape[0],1))\n",
    "        #y_train_inner = torch.reshape(y_train_inner, (y_train_inner.shape[0],1))\n",
    "\n",
    "\n",
    "        # iterate over hyperparameter h\n",
    "        for p, hidden_value_ in enumerate(h):\n",
    "            # define the model for each hidden layer\n",
    "            model_inner = lambda: torch.nn.Sequential(\n",
    "                        torch.nn.Linear(M, hidden_value_), #M features to H hiden units\n",
    "                        # 1st transfer function, either Tanh or ReLU:\n",
    "                        #torch.nn.Tanh(),                            \n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(hidden_value_, C),\n",
    "                        torch.nn.Softmax(dim=1)) #softmax as we want classes\n",
    "\n",
    "            # loss function for multinomial classification\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "            # Training the model\n",
    "            for r in range(n_replicates):\n",
    "                net_inner = model_inner()\n",
    "\n",
    "                torch.nn.init.xavier_uniform_(net_inner[0].weight)\n",
    "                torch.nn.init.xavier_uniform_(net_inner[2].weight)\n",
    "\n",
    "                optimizer = torch.optim.Adam(net_inner.parameters())\n",
    "\n",
    "                learning_curve = []\n",
    "                old_loss = 1e6\n",
    "                for i in range(max_iter):\n",
    "                    y_est = net_inner(X_train_inner)\n",
    "                    #y_class = torch.max(y_est, dim=1)[1]\n",
    "                    loss = loss_fn(y_est, y_train_inner)\n",
    "                    loss_value = loss.data.numpy()\n",
    "                    learning_curve.append(loss_value)\n",
    "\n",
    "\n",
    "                    p_delta_loss = np.abs(loss_value - old_loss)/old_loss\n",
    "                    if p_delta_loss < tolerance: break\n",
    "                    old_loss = loss_value\n",
    "\n",
    "\n",
    "                    #if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                    #    print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                    #    print(print_str)\n",
    "                    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "\n",
    "            softmax_logits = net_inner(X_test_inner)\n",
    "            y_test_est = (torch.max(softmax_logits, dim=1)[1]).data.numpy() \n",
    "            h_error_test[j, p]  = np.sum(y_test_est != np.array(y_test_inner))/len(y_test_inner)\n",
    "\n",
    "\n",
    "\n",
    "    #* Optimal values found from inner loop\n",
    "    #  the smallesst of the means of the differnet lambdas\n",
    "    opt_lambda[m] = lambda_interval[np.argmin(np.mean(lambda_error_test,axis=0))]\n",
    "    \n",
    "    opt_h[m] = h[np.argmin(np.mean(h_error_test, axis = 0))]\n",
    "\n",
    "\n",
    "    #* CLASSIFICATION\n",
    "    ##########################\n",
    "    ## BASELINE\n",
    "    # find out which class has the highest count\n",
    "    big_class = np.argmax([np.count_nonzero(y_train == i) for i in range(1,7)])\n",
    "    # assign everything to this class (calculate error rate)\n",
    "    base_line_train_err[m] = np.sum(y_train != big_class)/len(y_train)\n",
    "    base_line_test_err[m] = np.sum(y_test != big_class)/len(y_test)\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ## Logistic Regression\n",
    "    log = LogisticRegression(penalty='l2', C=1/opt_lambda[m], max_iter=1000)\n",
    "    log.fit(X_train, y_train)\n",
    "    y_train_est = log.predict(X_train)\n",
    "    y_test_est_log = log.predict(X_test)\n",
    "\n",
    "    log_train_error[m] = np.sum(y_train_est != y_train)/len(y_train)\n",
    "    log_test_error[m] = np.sum(y_test_est_log != y_test)/len(y_test)\n",
    "\n",
    "    ##########################\n",
    "    ## ANN-Classification\n",
    "\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, int(opt_h[m])), #M features to H hiden units\n",
    "                    # 1st transfer function, either Tanh or ReLU:\n",
    "                    #torch.nn.Tanh(),                            \n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(int(opt_h[m]),int(opt_h[m])),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(int(opt_h[m]), C), # H hidden units to 1 output neuron\n",
    "                    torch.nn.Softmax(dim=1) # final tranfer function\n",
    "                    )\n",
    "\n",
    "\n",
    "    X_train, y_train = torch.tensor(X_train, dtype = torch.float), torch.tensor(y_train)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype = torch.float), torch.tensor(y_test)\n",
    "    n_replicates = 3\n",
    "    for r in range(n_replicates):\n",
    "        net = model()\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(net[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(net[2].weight)\n",
    "\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "        learning_curve = []\n",
    "        old_loss = 1e6\n",
    "        for i in range(max_iter):\n",
    "            y_est = net(X_train)\n",
    "            #y_class = torch.max(y_est, dim=1)[1]\n",
    "            loss = loss_fn(y_est, y_train)\n",
    "            loss_value = loss.data.numpy()\n",
    "            learning_curve.append(loss_value)\n",
    "\n",
    "\n",
    "            p_delta_loss = np.abs(loss_value - old_loss)/old_loss\n",
    "            if p_delta_loss < tolerance: break\n",
    "            old_loss = loss_value\n",
    "\n",
    "\n",
    "            # if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                #print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                ###print(print_str)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        #print('\\t\\tFinal Loss:')\n",
    "        #print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "        ##print(print_str)\n",
    "\n",
    "        if loss_value < best_final_loss:\n",
    "            best_net = net\n",
    "            best_final_loss = loss_value\n",
    "            best_learning_curve = learning_curve\n",
    "        \n",
    "    \n",
    "    softmax_logits = net(X_test)\n",
    "    y_test_est_ann = (torch.max(softmax_logits, dim=1)[1]).data.numpy() \n",
    "    ANN_error_test[m] = np.sum(y_test_est_ann != np.array(y_test))/len(y_test)\n",
    "\n",
    "    ## STATISTICAL DATA\n",
    "    ##MCNEMAR DATA\n",
    "    data = np.array([y_test, y_test_est_ann, y_test_est_ann, (y_test == big_class)])\n",
    "    mc_nemar.append(data)\n",
    "    #nvm ; variance[m] = np.var(x_test)\n",
    "    # (y_test)\n",
    "    # (y_test_est_ann)\n",
    "    # (y_test_est_log)\n",
    "    # (y_test == big_class)\n",
    "\n",
    "# WARNING: Inden du kligger på den flotte trekant.\n",
    "# Det tager cirka 145 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold i    | h               E_ANN           lambda          E_log           E_base         \n",
      "-----------------------------------------------------------------------------------------\n",
      "1               | 9               0.455           0.599484        0.455           0.7272727      \n",
      "2               | 9               0.318           0.046416        0.318           0.5454545      \n",
      "3               | 7               0.227           0.599484        0.273           0.6363636      \n",
      "4               | 10              0.318           0.046416        0.318           0.8181818      \n",
      "5               | 10              0.238           0.599484        0.286           0.7142857      \n",
      "6               | 8               0.381           0.046416        0.381           0.7142857      \n",
      "7               | 8               0.381           0.046416        0.571           0.5714286      \n",
      "8               | 9               0.381           0.003594        0.333           0.6666667      \n",
      "9               | 10              0.476           0.046416        0.429           0.7142857      \n",
      "10              | 6               0.238           0.000022        0.571           0.6190476      \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} | {:<15} {:<15} {:<15} {:<15} {:<15}\".format('Outer fold i', 'h', 'E_ANN', 'lambda', 'E_log', 'E_base'))\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(\"{:<15} | {:<15} {:<15.3f} {:<15.6f} {:<15.3f} {:<15.7f}\".format(i+1, int(opt_h[i]), ANN_error_test[i], opt_lambda[i], log_test_error[i], base_line_test_err[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22., 22., 22., 22., 21., 21., 21., 21., 21., 21.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "table = np.array([n_list, opt_h, ANN_error_test, opt_lambda, log_test_error, base_line_test_err])\n",
    "\n",
    "df = pd.DataFrame(np.transpose(table))\n",
    "\n",
    "df.to_csv('classification_results.csv', index = False, header = [\"test_size\", \"h\", \"ANN\", \"lambda\", \"Log\", \"Base\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikolaj/Desktop/GIT/MLDM-Assignment-2/classification.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikolaj/Desktop/GIT/MLDM-Assignment-2/classification.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(ANN_error_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolaj/Desktop/GIT/MLDM-Assignment-2/classification.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(log_test_error)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolaj/Desktop/GIT/MLDM-Assignment-2/classification.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(base_line_test_err)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(ANN_error_test)\n",
    "plt.plot(log_test_error)\n",
    "plt.plot(base_line_test_err)\n",
    "#plt.axhline(0.5,color='red')\n",
    "plt.legend(['ANN', 'Log', 'Base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOGISTIC REGRESSION (5c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports :\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('fivethirtyeight')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "filename = 'data/glass.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# defining x and y for the linear regression\n",
    "#       all attributes except RI\n",
    "X = np.array(df.iloc[:,1:10])\n",
    "#       number of attributes\n",
    "M = 9\n",
    "#       type\n",
    "y = np.array(df.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# the format of C needs to be this, so it is usable with torch, see before and after:\n",
    "print(y)\n",
    "y[y>3] = y[y>3] - 1\n",
    "y = y - 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaj/opt/anaconda3/envs/AndSem/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "K_outer = 5\n",
    "\n",
    "\n",
    "lambda_interval = np.logspace(-8, 2, 20)\n",
    "log_train_error = np.zeros(K_outer)\n",
    "log_test_error = np.zeros(K_outer)\n",
    "\n",
    "opt_lambda = np.zeros(K_outer)\n",
    "\n",
    "max_iter = 100000\n",
    "\n",
    "\n",
    "cv_outer = model_selection.KFold(n_splits=K_outer,shuffle=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv_outer.split(X = X,y = y)):\n",
    "    X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # standardize\n",
    "    mu, sigma = np.mean(X_train), np.std(X_train)\n",
    "    X_train, X_test = (X_train - mu)/sigma, (X_test - mu)/sigma\n",
    "\n",
    "    # finding the best lambda (rlr validate but with logistic regression)\n",
    "    K_inner = 5\n",
    "\n",
    "    lambda_error_train = np.zeros((K_inner, len(lambda_interval)))\n",
    "    lambda_error_test = np.zeros((K_inner, len(lambda_interval)))\n",
    "\n",
    "\n",
    "    cv_inner = model_selection.KFold(n_splits=K_inner,shuffle=True)\n",
    "    for j, (train_index_inner, test_index_inner) in enumerate(cv_outer.split(X = X_train,y = y_train)):\n",
    "        X_train_inner, X_test_inner = X[train_index_inner, :], X[test_index_inner, :]\n",
    "        y_train_inner, y_test_inner = y[train_index_inner], y[test_index_inner]\n",
    "\n",
    "\n",
    "        for k in range(0, len(lambda_interval)):\n",
    "            #'newton-cg' gives better results\n",
    "            #mdl = LogisticRegression(penalty='l2', solver='newton-cg', C=1/lambda_interval[k], max_iter=100)\n",
    "            # but the other is faster but outputs hella lot of warnings:\n",
    "            mdl = LogisticRegression(penalty='l2', C=1/lambda_interval[k], max_iter=max_iter)\n",
    "            \n",
    "            mdl.fit(X_train_inner, y_train_inner)\n",
    "\n",
    "            y_train_est_inner = mdl.predict(X_train_inner).T\n",
    "            y_test_est_inner = mdl.predict(X_test_inner).T\n",
    "            \n",
    "            lambda_error_train[j,k] = np.sum(y_train_est_inner != y_train_inner) / len(y_train_inner)\n",
    "            lambda_error_test[j,k] = np.sum(y_test_est_inner != y_test_inner) / len(y_test_inner)\n",
    "\n",
    "    opt_lambda[i] = lambda_interval[np.argmin(np.mean(lambda_error_test, axis = 0))]\n",
    "\n",
    "    train_err_vs_lambda = np.mean(lambda_error_train,axis=0)\n",
    "    test_err_vs_lambda = np.mean(lambda_error_test,axis=0)\n",
    "\n",
    "    log = LogisticRegression(penalty='l2', C=1/opt_lambda[i], max_iter=max_iter)\n",
    "    log.fit(X_train, y_train)\n",
    "    y_train_est = log.predict(X_train)\n",
    "    y_test_est_log = log.predict(X_test)\n",
    "\n",
    "    log_train_error[i] = np.sum(y_train_est != y_train)/len(y_train)\n",
    "    log_test_error[i] = np.sum(y_test_est_log != y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39534884 0.41860465 0.39534884 0.37209302 0.4047619 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(log_test_error)\n",
    "print(log_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambda_interval, train_err_vs_lambda, 'r')\n",
    "plt.plot(lambda_interval, test_err_vs_lambda, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.15443469e-05, 1.66810054e-06, 1.00000000e-08, 4.64158883e-02,\n",
       "       1.00000000e-08])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('AndSem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d62badb4c3b216db4db7804bb7696037312a1bbdb2c7465cfb03cc785a825525"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
